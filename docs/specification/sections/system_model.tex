\section{System model}%TODO queue out of database 
	\subsection{System structure}
			\subsubsection{Modules}
The dynamic scheduler for scientific simulations follows these design architectures: \begin{itemize}
	\item {Master-Worker}
	\item {Task Stealing}
\end{itemize}

			\subsubsection{Master-Worker}
%The 'Master', the 'ExecutorWorker' and the 'Database' are subclasses of the Executor. Every instance of these classes runs as an independent MPI process, communicating via MPI messages. Therefore the minimum number of processes required to schedule the computation is three MPI nodes \left(one of each \right). Additional nodes will perform the 'ExecuterWorker' process, 'Master' and 'Database' nodes are singular.
The master class inheriting from 'AbstractScheduler' manages the simulation. It keeps a list of free workers and uses a scheduling strategy like FIFO to schedule tasks from the scientific code. While the task queue defined in the scheduling strategy is not empty, the master pops the next task, looks for a free worker to pass the task to, who will then execute it.
During the execution, in case a new scientific task appears, the worker sends it to the master, who adds the new task to the queue, according to the chosen policy. The worker also notifies the Database of all events occurring $($p.e. task appears, task started, task finished, â€¦$)$ who tracks them in a bookkeeping database.
After finishing executing a task, the worker notifies the master to add him to the list of free workers. The worker also provides the runtime of the task to the Database to store it in the statistic database for further usage $($data mining, plotting, etc.$)$.
If statistic based scheduling strategies $($SJF, LJF$)$ are chosen, the master calls the 'Database' to estimate a task's runtime in her data mining module, which is updated continuously during the simulation.
After the entire simulation process, the master sends a stop message to the other process in the MPI\_World communicator to exit the program.

			\subsubsection{Task Stealing}
In this architecture, one instance of the 'TaskstealingScheduler' and the 'Worker' class run on a single MPI node. One notifying the other in this case is implemented by calling the other's functions. In addition a singular 'Database' node is used. Therefore the minimum number of nodes required for scheduling the simulation is two MPI nodes. Calculation and database nodes communicate via MPI messages. Calculation nodes use MPI one sided communication to inform each other.
'TaskstealingScheduler' instances work fully independend from one another. While their queue is not empty they pop the next task and give it to their worker. If the queue is empty, a 'TaskstealingScheduler' will try to steal a scientific task from another 'TaskstealingScheduler' and give it to his worker.
The worker performs the task. He notifies the Database of all events occurring $($p.e. task appears, task started, task finished, ... $)$, who tracks them in a bookkeeping database. If a new task appears the  worker also calls the 'TaskstealingScheduler' to insert the task to the scheduling queue.
After completion the worker informs the corresponding scheduler that he's done and provides the runtime of the task to the Database to store it in the statistic database for further usage $($data mining, plotting, etc. $)$.
If statistic based scheduling strategies $($SJF, LJF $)$ are chosen, the 'TaskstealingScheduler' calls 'Database' to estimate a task's runtime in her data mining module, which is updated continuously during the simulation.
After the entire simulation process, the master sends a stop message to the other process in the MPI\_World communicator to exit the program.

\newpage
\subsection{Scenarios}
			
\subsubsection{Expanding universe simulation}

Jimmy Ellmersino, a scientist at Karlsruhe Institute of Technologie, wrote some computer code to simulate the physics of mass in an expanding universe. In order to track trillions of particles in the universe, the code should be executed on a HPC in an organized and smart manner. 
Firstly, Jimmy reserves the required number of nodes using the MOAB Workload Manager. After receiving the specific number of nodes, Jimmy runs the scheduler and chooses whether he wants to have multiple-queue or single queue scheduling. The scheduler initializes the scientific code and starts to schedule the initial tasks. The scheduler strives to keep as many processes as possible running at the same time and get
their results as soon as possible. After the simulation is complete Jimmy checks the scheduling performance by taking a look at the diagrams plotted by the visualization tool.

\subsection{Use cases}

\subsubsection{Scheduling system - Activity diagram}
	\begin{figure}[H]
		\includegraphics[width=1.5\textwidth,scale=0.75,trim=7cm 0 -7cm 0]{images/usecasediagram.png}
		\caption{Use case diagram}
	\end{figure}
	\begin{description}
\item [1. Brief Description]\hfill \\
This diagram describes a computation's use case general flow with the scheduler system.

\item [2. Actors]\hfill \\
\vspace{-6.5mm}
	\begin{enumerate}
		\item Code Interface
		\item Executer Interface
		\item MPI Interface
	\end{enumerate}

\newpage
\item [3. Preconditions]\hfill \\
\vspace{-6.5mm}
\begin{itemize}
\item Call of the code interface
\item MPI interaction

\end{itemize}

\item [4. Basic Flow of Event]\hfill \\
The scientific code for the computation will be provided from the code interface, so the use case "scientific code" extends the function of the scheduler. After binding the code on the scheduler and the interaction with the MPI world the computation can be placed through the executer interface. The use case "analyze data" is analyzing the data of the database and places the task in the queue. This is the use case "check queue/task placing". The function of the scheduler to use the database is represented through the use case "take/place data".


\item [5. Post-conditions]\hfill \\
\vspace{-6.5mm}
\begin{enumerate}
\item Call for execution on executer interface
\end{enumerate}

\item [6. Special requirements]\hfill \\
\vspace{-6.5mm}
None

\end{description}

\newpage


\newpage
\subsection{Object model}

\vspace{0.5cm}
\subsubsection{Scheduler modules and system events}
\vspace{0.5cm}
The main modules of the project are the following:
\begin{itemize}
	\item Scheduler
		\subitem Contains a variety of scheduling algorithms and strategies
		\subitem Queries Data Mining module how much resources the task needs
		\subitem Decides how to place a task in a schedule
		\subitem Sends bookkeeping notifications for the following events:
			\subsubitem - task appears
			\subsubitem - task started
			\subsubitem - task finished
			\subsubitem - interconnection between processes
	\item Data Mining
		\subitem Analyzes the statistics to define resource requirements for a task
		\subitem Stores statistic for completed tasks in the database
	
	\item Code interface
		\subitem The code interface provides following functions:
			\subsubitem place\_task(task)
			
	\item Executer interface
		\subitem The executer interface provides following functions:
			\subsubitem code\_preprocessing\_slave(void):void
			\subsubitem code\_postprocessing\_master(void):void
			\subsubitem code\_postprocessing\_slave(void):void
			\subsubitem run\_task(task):run result
	
	\item Database
		\subitem Keeps statistics, stores bookkeeping
		
	\item Communicator (Optional)
		\subitem Has to organize multi-level communication among multiple parallel schedulers
\end{itemize}
\vspace{1cm}
The picture below describes the basic object model of the dynamic scheduler, its modules and the interaction (system events) in between.

\vspace{1cm}
\begin{figure}[H]
	\includegraphics[width=.9\textwidth,height=.9\textheight,keepaspectratio]	{images/modules.jpg}
	\caption{Basic description of modules}
\end{figure}
\newpage

\subsection{Dynamic models}
\subsubsection{Task Stealing - activity diagram with textual description}
%TODO must have
\vspace{1.5cm}
\begin{figure}[H]
	\includegraphics[width=15cm]{images/task_stealing_pap.jpg}
	\caption{Activity diagram of the task stealing algorithm (Source: PSE-Scheduling.pdf)}
\end{figure}

\begin{description}
	\item [1. Brief Description]\hfill \\
	Use case describing the way the task stealing algorithm works inside the dynamic scheduler in order to reduce idle time of all processors.
	
	\item [2. Actors]\hfill \\
	No actors. 
	
	
	\item [3. Preconditions]\hfill \\
	\vspace{-6.5mm}
	\begin{itemize}
		\item The scheduler has been initialized and it has already created a MPI memory window for RMA (Remote Memory Access).
		\item Information about initial tasks has been collected from code preprocessing, received tasks from which have been placed to the queue.
	\end{itemize}
	
	\item [4. Basic Flow of Events]\hfill \\
	The use case begins when the scheduler checks if there are tasks in its own queue. If there is a task in the queue, it will be executed. After the task has been executed, all collected results will get placed in the database in an organized manner.
	If there isn't any task in the queue, the task stealing process will be initiated.\newline First of all, the scheduler will cycle through all running processes in other processors and try to steal an idle task from them. In order to check the other processes' queue for an idle task, the scheduler tries to block the RMA window for offsets and statuses, after which it checks whether there are any idle tasks. If the queue is empty or if the RMA window is currently blocked, the scheduler repeats the same procedure for remaining processors. If after checking all processes, no idle task was found, the scheduler changes its own state to idle. If this idle state doesn't change for a specific amount of cycles, which means that no tasks were stolen in this period, the scheduler finally checks to make sure that no process is currently running in any processor and if that's the case, it finishes its own routine. 
	
	
	\item [5. Post-conditions]\hfill \\
	\vspace{-6.5mm}
	\begin{enumerate}
		\item The scheduler frees the allocated MPI memory window for RMA.
		\item The scheduler executes the finishing post processing routine, where it collects information from the whole process (e.g. global total run time).
	\end{enumerate}
	
	\item [6. Special requirements]\hfill \\
	\vspace{-6.5mm}
	\begin{enumerate}
		\item The scheduler should have access to the executer interface.
		\item The scheduler should be accessible through an interface, in order to receive new tasks from the code interface.
	\end{enumerate}
	
\end{description}
\vspace{0.5cm}
\subsubsection{Single-queue scheduling managed by master/slave design }
\vspace{0.5cm}
A single-queue master-managed scheduling is a way to schedule many tasks, where each process (instance of the task) can request master to access the queue. The picture below describes the program flow of a scheduler by a single queue, where the queue is managed by only one process (master). 
\vspace{1cm}
	\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{images/Master-slave.jpg}
	\caption{Single queue scheduling with a master-slave diagram} 
	\end{figure}
\newpage
\subsubsection{Multiple-queues scheduling}
%TODO unite with taskstealing paragraphs (must have)	

\vspace{0.5cm}
A multiple-queues scheduling is a way to schedule many tasks, where each process has its own queue, and does the same task. The picture below describes the program flow of a scheduler designed by multiple queues, where the queue managing is based on task-stealing algorithm.
\vspace{1cm}
	\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{images/Task-stealing.jpg}
	\caption{Multiple-queues scheduling managed by task-stealing algorithm}
	\end{figure}
\newpage
	
\subsection{Graphical user interface}
	\subsubsection{Command line parameters}
		The dynamic scheduler is started via the command line, taking the following parameters as command line parameters:
	
		\begin{itemize}	
			\item Syntax: [OPTION]... [RUN ARGUMENTS]...
				
			\item OPTIONS:
							
				\subitem -design: choose between multiple-queue scheduling (multi-queue) and Master-Worker scheduling (master-worker) (e.g. -d multi-queue or -d master-worker). The default strategy is master-worker (nice-to-have)
				%TODO spelling				
				
				\subitem -strategy: choose between several scheduling algorithms or automatic dynamic scheduling (auto) (e.g. -s fifo or -s auto). Default is automatic dynamic scheduling.
					
			\item RUN ARGUMENTS:
				\subitem The run arguments is passed to the scientific code for initialization
		\end{itemize}
		
	\subsubsection{Graphical user interface (GUI) (optional)}
		The Graphical user interface (GUI) is a optional feature. Scientists should be able to easily generate a shell script running via MOAB Workload Manager on the HPC. The following configuration can be edited via GUI:
		
		\begin{itemize}
			\item Scheduling strategy
			\item Number of nodes
			\item Number of processes per node
			\item Wall-clock time. Default units are seconds. 
			HH:MM:SS format is also accepted.
			\item Maximum amount of physical memory used by any single process of the job. Allowed units are kb, mb, gb. Be aware that processes are either MPI tasks if running MPI parallel jobs or threads if running multithreaded jobs.
			\item Maximum amount of physical memory used by the job.
			Allowed units are kb, mb, gb. Be aware that this memory value is the accumulated memory for all MPI tasks or all threads of the job.
			\item Run Arguments for the scientific code
			\item User specified name for the job.
			\item Defines the queue class
			\item Expand the list of environment variables that are exported to the job
			\item Send email to the specified email address when job begins, ends or aborts
		\end{itemize}
	
	